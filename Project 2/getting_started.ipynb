{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project II: Economic Growth \n",
    "\n",
    "This notebook will help you getting started with analyzing the growth dataset, `growth.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as la\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('growth.csv')\n",
    "lbldf = pd.read_csv('labels.csv', index_col='variable')\n",
    "lbl_all = lbldf.label.to_dict() # as a dictionary\n",
    "print(f'The data contains {dat.shape[0]} rows (countries) and {dat.shape[1]} columns (variables).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.plot.scatter(x='lgdp_initial', y='gdp_growth');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "sns.scatterplot(x='lgdp_initial', y='gdp_growth', data=dat, hue='malfal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collections of variables\n",
    "\n",
    "In order to make the analysis simpler, it may be convenient to collect variables in sets that belong together naturally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all available variables\n",
    "vv_institutions = ['marketref', 'dem', 'demCGV', 'demBMR', 'demreg'] \n",
    "\n",
    "vv_geography = [\n",
    "        'tropicar','distr', 'distcr', 'distc','suitavg','temp', 'suitgini', 'elevavg', 'elevstd',\n",
    "        'kgatr', 'precip', 'area', 'abslat', 'cenlong', 'area_ar', 'rough','landlock', \n",
    "        'africa',  'asia', 'oceania', 'americas' # 'europe' is the reference\n",
    "]\n",
    "\n",
    "vv_geneticdiversity = ['pdiv', 'pdiv_aa', 'pdivhmi', 'pdivhmi_aa']\n",
    "vv_historical = ['pd1000', 'pd1500', 'pop1000', 'pop1500', 'ln_yst'] # these are often missing: ['pd1', 'pop1']\n",
    "vv_religion = ['pprotest', 'pcatholic', 'pmuslim']\n",
    "vv_danger = ['yellow', 'malfal',  'uvdamage']\n",
    "vv_resources = ['oilres', 'goldm', 'iron', 'silv', 'zinc']\n",
    "vv_educ = ['ls_bl', 'lh_bl'] # secondary, tertiary: we exclude 'lp_bl' (primary) to avoid rank failure \n",
    "\n",
    "vv_all = {'institutions': vv_institutions, \n",
    "          'geography': vv_geography, \n",
    "          'geneticdiversity': vv_geneticdiversity,\n",
    "          'historical': vv_historical,\n",
    "          'religion': vv_religion,\n",
    "          'danger':vv_danger, \n",
    "          'resources':vv_resources\n",
    "         }\n",
    "\n",
    "list_of_lists = vv_all.values()\n",
    "vv_all['all'] = [v for sublist in list_of_lists for v in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenient to keep a column of ones in the dataset\n",
    "dat['constant'] = np.ones((dat.shape[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_func(y,X,logg):\n",
    "\n",
    "    # Find the shape of X\n",
    "    K = X.shape[1]\n",
    "\n",
    "    # Run OLS\n",
    "    coefs_OLS = la.inv(X.T @ X) @ X.T @ y\n",
    "    beta_OLS = coefs_OLS[logg][0]\n",
    "\n",
    "    # Save the residuals \n",
    "    res_OLS = y - X @ coefs_OLS\n",
    "\n",
    "    # Estimate variance\n",
    "    SSR = res_OLS.T @ res_OLS\n",
    "    sigma2_OLS = SSR/(X.shape[0] - K)\n",
    "    cov = sigma2_OLS * la.inv(X.T @ X)\n",
    "\n",
    "    # Calculate standard errors\n",
    "    se = np.sqrt(np.diag(cov)).reshape(-1,1)\n",
    "\n",
    "    # Get standard error of alpha\n",
    "    se_OLS = se[logg][0]\n",
    "\n",
    "    # Calculate the quantile of the standard normal distribution that corresponds to the 95% confidence interval of a two-sided test\n",
    "    q = norm.ppf(1-0.025)\n",
    "\n",
    "    # Calculate confidence interval\n",
    "    CI_OLS =  (((beta_OLS-q*se_OLS).round(2),(beta_OLS+q*se_OLS).round(2)))\n",
    "\n",
    "    # Returns beta, standard eroor and confidence interval\n",
    "    return beta_OLS, se_OLS, CI_OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoiding missings \n",
    "I = dat[['gdp_growth', 'lgdp_initial']].notnull().all(axis=1)\n",
    "\n",
    "# Extract dataset \n",
    "y = dat.loc[I, 'gdp_growth'].values.reshape((-1,1)) * 100.0\n",
    "X = dat.loc[I, ['constant','lgdp_initial']].values\n",
    "\n",
    "# Extract from OLS function \n",
    "beta_OLS_1, se_OLS_1, CI_OLS_1 = OLS_func(y,X,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zs = vv_all['geography'] + vv_all['religion'] + ['pop_growth', 'investment_rate'] + ['marketref', 'dem', 'demreg'] \n",
    "# ds = ['lgdp_initial']\n",
    "# xs = ds + zs\n",
    "\n",
    "zs = vv_all['geography'] + vv_all['religion'] + ['pop_growth', 'investment_rate','marketref', 'demreg', 'dem', 'currentinst','pdiv', 'lh_bl', 'lp_bl', 'ls_bl']\n",
    "ds = ['lgdp_initial']\n",
    "xs = ds + zs\n",
    "\n",
    "# Avoiding missings\n",
    "all_vars = ['gdp_growth'] + xs\n",
    "I = dat[all_vars].notnull().all(1)\n",
    "\n",
    "# Extract data\n",
    "X = dat.loc[I, xs].values\n",
    "Z = dat.loc[I, zs].values\n",
    "D = dat.loc[I, ds].values\n",
    "y = dat.loc[I,'gdp_growth'].values.reshape((-1,1)) * 100. #easier to read output when growth is in 100%\n",
    "\n",
    "# Add const. (unless this breaks the rank condition)\n",
    "oo = np.ones((I.sum(),1))\n",
    "XX = np.hstack([X, oo])\n",
    "xs.append('constant') # we put it in as the last element\n",
    "\n",
    "# Check the rank condition\n",
    "K = XX.shape[1]\n",
    "assert np.linalg.matrix_rank(XX) == XX.shape[1], f'X does not have full rank'\n",
    "\n",
    "# Extract from OLS function \n",
    "beta_OLS_2, se_OLS_2, CI_OLS_2 = OLS_func(y,XX,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding technical variables - interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add interaction terms \n",
    "int = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "Z_int = int.fit_transform(Z)\n",
    "\n",
    "# Removes all columns which have a std of 0 in order to fulfill the rank condition \n",
    "i_idx = [i for i in range(Z_int.shape[1]) if np.std(Z_int[:, i]) != 0]\n",
    "i_idx_2 = [i for i in range(Z_int.shape[1]) if np.std(Z_int[:, i]) == 0]\n",
    "\n",
    "# Dropping interaction terms without variance\n",
    "Z_int = Z_int[:,i_idx]    \n",
    "\n",
    "# Define X \n",
    "X_int = np.hstack([D,Z_int])\n",
    "\n",
    "# i_idx_x = [i for i in range(X_int.shape[1]) if np.std(X_int[:, i]) != 0]\n",
    "# int_names = int.get_feature_names_out(xs[:-2])\n",
    "# idx_x = int_names[i_idx_x] \n",
    "# print(idx_x)\n",
    "\n",
    "# Interaction terms for dummies at then excluded \n",
    "int_names = int.get_feature_names_out(zs)\n",
    "idx_2 = int_names[i_idx_2] \n",
    "print(idx_2)\n",
    "\n",
    "# Extract from OLS function \n",
    "beta_OLS_3, se_OLS_3, CI_OLS_3 = OLS_func(y,X_int,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding technical variables - Squared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add squared terms \n",
    "poly = PolynomialFeatures(2, include_bias=False)\n",
    "Z_poly = poly.fit_transform(Z)\n",
    "\n",
    "# Remores all columns which have a std of 0 in order to fulfill the rank condition \n",
    "i_idx = [i for i in range(Z_poly.shape[1]) if np.std(Z_poly[:, i]) != 0]\n",
    "i_idx_2 = [i for i in range(Z_poly.shape[1]) if np.std(Z_poly[:, i]) == 0]\n",
    "\n",
    "# Dropping interaction terms with out variance\n",
    "Z_poly = Z_poly[:,i_idx]      \n",
    "X_poly = np.hstack([D,Z_poly])\n",
    "\n",
    "# i_idx_x = [i for i in range(X_poly.shape[1]) if np.std(X_poly[:, i]) != 0]\n",
    "# int_names = poly_2.get_feature_names_out(xs[:-2])\n",
    "# idx_x = int_names[i_idx_x] \n",
    "# print(idx_x)\n",
    "\n",
    "# Interaction terms for dummies at then excluded \n",
    "int_names = poly.get_feature_names_out(zs)\n",
    "idx_2 = int_names[i_idx_2] \n",
    "print(idx_2)\n",
    "\n",
    "# Extract from OLS function \n",
    "beta_OLS_4, se_OLS_4, CI_OLS_4 = OLS_func(y,X_poly,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standardize function \n",
    "def standardize(X):\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_std = np.std(X, axis=0)\n",
    "    X_stan = (X - X_mean)/X_std\n",
    "    return X_stan\n",
    "\n",
    "# Standardize all\n",
    "X_stan = standardize(X)\n",
    "Z_stan = standardize(Z)\n",
    "D_stan = standardize(D)\n",
    "Z_int_stan = standardize(Z_int)\n",
    "X_int_stan = standardize(X_int)\n",
    "Z_poly_stan = standardize(Z_poly)\n",
    "X_poly_stan = standardize(X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to define penalty term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function that calculates BRT\n",
    "def BRT(X_tilde,y):\n",
    "    n,p = X_tilde.shape\n",
    "    sigma = np.std(y, ddof=1)\n",
    "    c = 1.1\n",
    "    alpha = 0.05\n",
    "    max_term = 1  \n",
    "    penalty_BRT = c*sigma*norm.ppf(1-alpha/(2*p))/np.sqrt(n*max_term)\n",
    "\n",
    "    return penalty_BRT\n",
    "\n",
    "# Make a function that calculates BCCH\n",
    "def BCCH(X_tilde,y):\n",
    "    n,p = X_tilde.shape\n",
    "    c = 1.1\n",
    "    alpha = 0.05\n",
    "    yXscale = (np.max((X_tilde.T ** 2) @ ((y-np.mean(y)) ** 2) / n)) ** 0.5\n",
    "    penalty_pilot = c / np.sqrt(n) * norm.ppf(1-alpha/(2*p)) * yXscale # Note: Have divided by 2 due to Python definition of Lasso\n",
    "\n",
    "    # Pilot estimates\n",
    "    coef_pilot = Lasso(alpha=penalty_pilot, max_iter=10_000).fit(X_tilde,y).coef_\n",
    "    coef_intercept = Lasso(alpha=penalty_pilot, max_iter=10_000).fit(X_tilde,y).intercept_\n",
    "    pred = (coef_intercept + X_tilde@coef_pilot)\n",
    "\n",
    "    # Updated penalty\n",
    "    eps = y - pred \n",
    "    epsXscale = (np.max((X_tilde.T ** 2) @ (eps ** 2) / n)) ** 0.5\n",
    "    penalty_BCCH = c*norm.ppf(1-alpha/(2*p))*epsXscale/np.sqrt(n)\n",
    "\n",
    "    return penalty_BCCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Double Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Lasso Y using D and Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating penalty terms\n",
    "penalty_BRTyx = BRT(X_stan,y)\n",
    "penalty_BCCHyx = BCCH(X_stan,y)\n",
    "penalty_BRTyx_1 = BRT(X_int_stan,y)\n",
    "penalty_BCCHyx_1 = BCCH(X_int_stan,y)\n",
    "penalty_BRTyx_2 = BRT(X_poly_stan,y)\n",
    "penalty_BCCHyx_2 = BCCH(X_poly_stan,y)\n",
    "\n",
    "# Lasso on gqp growth\n",
    "fit_BRTyx = Lasso(alpha=penalty_BRTyx, max_iter=10_000).fit(X_stan,y) \n",
    "fit_BCCHyx = Lasso(alpha=penalty_BCCHyx, max_iter=10_000).fit(X_stan,y) \n",
    "fit_BRTyx_1 = Lasso(alpha=penalty_BRTyx_1, max_iter=10_000).fit(X_int_stan,y) \n",
    "fit_BCCHyx_1 = Lasso(alpha=penalty_BCCHyx_1, max_iter=10_000).fit(X_int_stan,y) \n",
    "fit_BRTyx_2 = Lasso(alpha=penalty_BRTyx_2, max_iter=10_000).fit(X_poly_stan,y) \n",
    "fit_BCCHyx_2 = Lasso(alpha=penalty_BCCHyx_2, max_iter=10_000).fit(X_poly_stan,y) \n",
    "\n",
    "# Lasso coefficients\n",
    "coefs = fit_BRTyx.coef_\n",
    "coefs_BCCH = fit_BCCHyx.coef_\n",
    "coefs_1 = fit_BRTyx_1.coef_\n",
    "coefs_BCCH_1 = fit_BCCHyx_1.coef_\n",
    "coefs_2 = fit_BRTyx_2.coef_\n",
    "coefs_BCCH_2 = fit_BCCHyx_2.coef_\n",
    "\n",
    "# save residuals\n",
    "resyx = y - fit_BRTyx.predict(X_stan) \n",
    "resyx_BCCH = y - fit_BCCHyx.predict(X_stan)\n",
    "resyx_1 = y - fit_BRTyx_1.predict(X_int_stan)\n",
    "resyx_BCCH_1 = y - fit_BCCHyx_1.predict(X_int_stan)\n",
    "resyx_2 = y - fit_BRTyx_2.predict(X_poly_stan)\n",
    "resyx_BCCH_2 = y - fit_BCCHyx_2.predict(X_poly_stan)\n",
    "\n",
    "# save residuals\n",
    "resyxz = resyx + D_stan * coefs[0]\n",
    "resyxz_BCCH = resyx_BCCH + D_stan * coefs_BCCH[0]\n",
    "resyxz_1 = resyx_1 + D_stan * coefs_1[0]\n",
    "resyxz_BCCH_1 = resyx_BCCH_1 + D_stan * coefs_BCCH_1[0]\n",
    "resyxz_2 = resyx_2 + D_stan * coefs_2[0]\n",
    "resyxz_BCCH_2 = resyx_BCCH_2 + D_stan * coefs_BCCH_2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Lasso D using Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating penalty terms\n",
    "penalty_BRTdz = BRT(Z_stan, D)\n",
    "penalty_BCCHdz = BCCH(Z_stan, D)\n",
    "penalty_BRTdz_1 = BRT(Z_int_stan, D)\n",
    "penalty_BCCHdz_1 = BCCH(Z_int_stan, D)\n",
    "penalty_BRTdz_2 = BRT(Z_poly_stan, D)\n",
    "penalty_BCCHdz_2 = BCCH(Z_poly_stan, D)\n",
    "\n",
    "# Lasso on initial gdp\n",
    "fit_BRTdz = Lasso(alpha = penalty_BRTdz).fit(Z_stan,D) \n",
    "fit_BCCHdz = Lasso(alpha = penalty_BCCHdz).fit(Z_stan,D)\n",
    "fit_BRTdz_1 = Lasso(alpha = penalty_BRTdz_1).fit(Z_int_stan,D) \n",
    "fit_BCCHdz_1 = Lasso(alpha = penalty_BCCHdz_1).fit(Z_int_stan,D)\n",
    "fit_BRTdz_2 = Lasso(alpha = penalty_BRTdz_2).fit(Z_poly_stan,D) \n",
    "fit_BCCHdz_2 = Lasso(alpha = penalty_BCCHdz_2).fit(Z_poly_stan,D)\n",
    "\n",
    "# Save residuals\n",
    "resdz = D - fit_BRTdz.predict(Z_stan)\n",
    "resdz_BCCH = D - fit_BCCHdz.predict(Z_stan)\n",
    "resdz_1 = D - fit_BRTdz_1.predict(Z_int_stan)\n",
    "resdz_BCCH_1 = D - fit_BCCHdz_1.predict(Z_int_stan)\n",
    "resdz_2 = D - fit_BRTdz_2.predict(Z_poly_stan)\n",
    "resdz_BCCH_2 = D - fit_BCCHdz_2.predict(Z_poly_stan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Estimate $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_PDL_func(resdz,resyxz,d):\n",
    "    num = np.sum(resdz*resyxz)\n",
    "    denom = np.sum(resdz*d)\n",
    "    beta_PDL = num/denom\n",
    "    return beta_PDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating Post Double Lasso\n",
    "beta_PDL = beta_PDL_func(resdz, resyxz, D)\n",
    "beta_PDL_BCCH = beta_PDL_func(resdz_BCCH, resyxz_BCCH, D)\n",
    "beta_PDL_1 = beta_PDL_func(resdz_1, resyxz_1, D)\n",
    "beta_PDL_BCCH_1 = beta_PDL_func(resdz_BCCH_1, resyxz_BCCH_1, D)\n",
    "beta_PDL_2 = beta_PDL_func(resdz_2, resyxz_2, D)\n",
    "beta_PDL_BCCH_2 = beta_PDL_func(resdz_BCCH_2, resyxz_BCCH_2, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step ...: Calculate standard errors and confidence interval for $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_PDL_CI_func(alpha_PDL,resdz,resyx):\n",
    "    # Variance\n",
    "\n",
    "    N = resyx.shape[0]\n",
    "    num = np.sum(resyx**2 * resdz**2) / N\n",
    "    denom = (np.sum(resdz**2) / N)**2\n",
    "    sigma2_PDL = num/denom\n",
    "\n",
    "    # Confidence interval\n",
    "    q = norm.ppf(1-0.025)\n",
    "    se_PDL = np.sqrt(sigma2_PDL) / np.sqrt(N)    # calculating standard error as the squareroot of the mean variance\n",
    "    CI_PDL=(((alpha_PDL-q*se_PDL).round(2),(alpha_PDL+q*se_PDL).round(2)))\n",
    "\n",
    "    return se_PDL, CI_PDL\n",
    "\n",
    "# estimating standard errors and confidence interval for Post Double Lasso\n",
    "se_PDL, CI_PDL = beta_PDL_CI_func(beta_PDL, resdz, resyxz)\n",
    "se_PDL_BCCH, CI_PDL_BCCH = beta_PDL_CI_func(beta_PDL_BCCH, resdz_BCCH, resyxz_BCCH)\n",
    "se_PDL_1, CI_PDL_1 = beta_PDL_CI_func(beta_PDL_1, resdz_1, resyxz_1)\n",
    "se_PDL_BCCH_1, CI_PDL_BCCH_1 = beta_PDL_CI_func(beta_PDL_BCCH_1, resdz_BCCH_1, resyxz_BCCH_1)\n",
    "se_PDL_2, CI_PDL_2 = beta_PDL_CI_func(beta_PDL_2, resdz_2, resyxz_2)\n",
    "se_PDL_BCCH_2, CI_PDL_BCCH_2 = beta_PDL_CI_func(beta_PDL_BCCH_2, resdz_BCCH_2, resyxz_BCCH_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimates\n",
    "\n",
    "estimates = np.array([beta_OLS_1, beta_OLS_2, beta_OLS_3, beta_OLS_4, beta_PDL, beta_PDL_1, beta_PDL_2, beta_PDL_BCCH, beta_PDL_BCCH_1, beta_PDL_BCCH_2]).round(4)\n",
    "label_over_column = ['(1)','(2)','(3)','(4)','(5)','(6)','(7)','(8)','(9)','(10)']\n",
    "label_column = np.array(['OLS', 'OLS', 'OLS', 'OLS', 'PDL', 'PDL', 'PDL', 'PDL', 'PDL', 'PDL'])\n",
    "label_row = ['' ,'Initial $\\log{(gdp)}$', 'se', 'No controls','No obs','$\\lambda^{dz}$','$\\lambda^{yx}$','t-statistic']\n",
    "se = np.array([se_OLS_1, se_OLS_2, se_OLS_3, se_OLS_4, se_PDL, se_PDL_1, se_PDL_2, se_PDL_BCCH, se_PDL_BCCH_1, se_PDL_BCCH_2]).round(4)\n",
    "no_controls = np.array([0, 0, 0, len(xs) - 1, len(zs), Z_int_stan.shape[1],  Z_poly_stan.shape[1], len(zs), Z_int_stan.shape[1], Z_poly_stan.shape[1]])\n",
    "no_obs = np.array([len(y), len(y), len(y), len(y), len(y), len(y), len(y), len(y), len(y), len(y)])\n",
    "\n",
    "pens_dz = np.array(['', '', '', '', penalty_BRTdz.round(4), penalty_BRTdz_1.round(4), penalty_BRTdz_2.round(4), penalty_BCCHdz.round(4), penalty_BCCHdz_1.round(4), penalty_BCCHdz_2.round(4)])\n",
    "pens_yx = np.array(['', '', '', '', penalty_BRTyx.round(4), penalty_BRTyx_1.round(4), penalty_BRTyx_2.round(4), penalty_BCCHyx.round(4), penalty_BCCHyx_1.round(4), penalty_BCCHyx_2.round(4)])\n",
    "\n",
    "t_statistic = (estimates/se).round(4)    \n",
    "\n",
    "data = np.row_stack((label_column ,estimates, se, no_controls,no_obs, pens_dz, pens_yx,t_statistic))\n",
    "\n",
    "df = pd.DataFrame(data = data, index = label_row, columns = label_over_column)\n",
    "\n",
    "print(df.to_latex(escape = False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
